{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nCreated on Jul 31.2019\\n机器学习实战之决策树源码\\n@author:Tobin\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# coding utf-8\n",
    "\"\"\"\n",
    "Created on Jul 31.2019\n",
    "机器学习实战之决策树源码+我的一些注释\n",
    "@author:Tobin\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算给定数据集的香农熵，熵定义为信息的期望值。即要写一个计算给定数据集信息量的代码。我们使用频率近似概率。\n",
    "# 数据集的形式是前n项为特征，最后一项为label值。使用一个字典记录下每个特征下数据的数量\n",
    "from math import log\n",
    "def calcShannonEnt(dataSet):\n",
    "    numEntries = len(dataSet)\n",
    "    labelCounts = {}\n",
    "    for featVec in dataSet:\n",
    "        currentLabel = featVec[-1]\n",
    "        if currentLabel not in labelCounts.keys():\n",
    "            labelCounts[currentLabel] = 0\n",
    "        labelCounts[currentLabel]+=1\n",
    "    shannonEnt = 0.0\n",
    "    for key in labelCounts:\n",
    "        prob = float(labelCounts[key])/numEntries\n",
    "        shannonEnt -= prob*log(prob, 2)\n",
    "    return shannonEnt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建数据集，使用calcShannonEnt函数，验证是否写正确\n",
    "def createDataSet():\n",
    "    dataSet = [[1, 1, 'yes'],\n",
    "               [1, 1, 'yes'],\n",
    "               [1, 0, 'no'],\n",
    "               [0, 1, 'no'],\n",
    "               [0, 1, 'no']]\n",
    "    labels = ['no surfacing','flippers']\n",
    "    #change to discrete values\n",
    "    return dataSet, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9709505944546686"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data, labels = createDataSet()\n",
    "calcShannonEnt(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 写完了计算信息量的函数，需要一个按照给定特征进行数据集分割的函数，然后我们计算分割后的信息量，\n",
    "# 再判断选择哪个特征划分数据集\n",
    "# axis表示特征，value表示特征值，则相当于将某特征符合某值的数据分割出来\n",
    "# 注意extend和append的区别，append将list作为元素插入，extend将两个list合并为一个list\n",
    "def splitDataSet(dataSet, axis, value):\n",
    "    retDataSet = []\n",
    "    for featVec in dataSet:\n",
    "        if featVec[axis] == value:\n",
    "            # 这里去除了当前特征\n",
    "            reducedFeatVec = featVec[:axis]    \n",
    "            reducedFeatVec.extend(featVec[axis+1:])\n",
    "            retDataSet.append(reducedFeatVec)\n",
    "    return retDataSet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 'yes'], [1, 'yes'], [0, 'no']]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 测试splitDataSet函数是否正确\n",
    "splitDataSet(data, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 现在我们需要遍历所有特征，计算每个特征划分后的信息增益，返回信息增益最大的特征\n",
    "def chooseBestFeatureToSplit(dataSet):\n",
    "    numFeatures = len(dataSet[0]) - 1     \n",
    "    baseEntropy = calcShannonEnt(dataSet)\n",
    "    bestInfoGain = 0.0; bestFeature = -1\n",
    "    # 遍历所有特征\n",
    "    for i in range(numFeatures):     \n",
    "        featList = [example[i] for example in dataSet]\n",
    "        uniqueVals = set(featList)    \n",
    "        newEntropy = 0.0\n",
    "        # 遍历该特征的所有特征值。\n",
    "        for value in uniqueVals:\n",
    "            subDataSet = splitDataSet(dataSet, i, value)\n",
    "            prob = len(subDataSet)/float(len(dataSet))\n",
    "            newEntropy += prob * calcShannonEnt(subDataSet)     \n",
    "        # 计算信息增益：熵 - 条件熵\n",
    "        infoGain = baseEntropy - newEntropy     \n",
    "        # 记录最大的信息熵， 和最大信息熵时候的特征。\n",
    "        if (infoGain > bestInfoGain):       \n",
    "            bestInfoGain = infoGain        \n",
    "            bestFeature = i\n",
    "    return bestFeature                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 测试chooseBestFeatureToSplit函数，使用自己构造的数据代码\n",
    "chooseBestFeatureToSplit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 现在已经有了计算信息量的函数，划分数据集的函数，和选取最好的划分特征的函数\n",
    "# 当我们划分完一次后，我们需要对剩下的各个分支继续进行这样的划分，直到满足递归条件\n",
    "# 递归结束的条件是程序遍历完数据集的属性，或者每个分支下的所有实例都具有相同的分类\n",
    "# 如果我们已经遍历完所有的属性，子结点中的数据还不是同属于同一类怎么办呢？此时使用多数表决的方式决定该子节点的分类。\n",
    "# 统计个类别的数量， 进行排序， 返回数量最多的类别。\n",
    "\n",
    "\n",
    "import operator\n",
    "def majorityCnt(classList):\n",
    "    classCount={}\n",
    "    for vote in classList:\n",
    "        if vote not in classCount.keys(): \n",
    "            classCount[vote] = 0\n",
    "        classCount[vote] += 1\n",
    "    # 因为python3中dict已经没有iteritems这个属性，直接改为items即可\n",
    "    # operator模块提供的itemgetter函数用于获取对象的哪些维的数据，参数为一些序号（即需要获取的数据在对象中的序号），下面看例子。\n",
    "    sortedClassCount = sorted(classCount.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    print(sortedClassCount)\n",
    "    return sortedClassCount[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 4), (2, 2), (3, 2), (4, 2)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# classList = [1,2,3,4,3,2,1,1,1,4]\n",
    "# majorityCnt(classList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 我们已经有了所有的子模块，下面就可以递归创建决策树了，现在我们的算法是ID3算法，对于数值型数据比较乏力\n",
    "# 树的结构只用字典进行存储\n",
    "# 递归创建决策树\n",
    "def createTree(dataSet,labels):\n",
    "    # 获取所有样本的类别\n",
    "    classList = [example[-1] for example in dataSet]\n",
    "    # 两个递归边界，所有样本属于同一类或者特征遍历完毕，ID3每次删去一个特征，最终只剩下一个特征\n",
    "    if classList.count(classList[0]) == len(classList): \n",
    "        return classList[0]#stop splitting when all of the classes are equal\n",
    "    # 如果特征已为空， 将类别数量最多的作为该节点的类别。\n",
    "    if len(dataSet[0]) == 1: #stop splitting when there are no more features in dataSet\n",
    "        return majorityCnt(classList)\n",
    "    # 选取最好的特征作为此时的节点。\n",
    "    bestFeat = chooseBestFeatureToSplit(dataSet)\n",
    "    # 通过索引获取到最好的特征标识，用于下面字典记录书的节点。\n",
    "    bestFeatLabel = labels[bestFeat]\n",
    "    # 决策树\n",
    "    myTree = {bestFeatLabel:{}}\n",
    "    # 将最好的特征删除掉， 该特征后边已经用不到了， 将递归的重新选取除此之外的下一个特征，最为树的节点。\n",
    "    del(labels[bestFeat])\n",
    "    # 基于最好的特征继续构建树。\n",
    "    featValues = [example[bestFeat] for example in dataSet]\n",
    "    uniqueVals = set(featValues)\n",
    "    # 遍历最好特征的每一个特征值继续构建树。\n",
    "    for value in uniqueVals:\n",
    "        subLabels = labels[:]      \n",
    "        myTree[bestFeatLabel][value] = createTree(splitDataSet(dataSet, bestFeat, value),subLabels)\n",
    "    return myTree   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 验证建树结果\n",
    "data, labels = createDataSet()\n",
    "myTree = createTree(data, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 决策树的一个好处时比较直观，书中还有可视化决策树的代码，但不是学习重点，在次略过\n",
    "# 下面我们要使用构建好的决策树对测试数据进行预测，对照上面的验证结果更好理解，在递归地访问叶节点的过程中，featLabels和testVec是不变的\n",
    "def classify(inputTree,featLabels,testVec):\n",
    "    # 获取树的第一个keys\n",
    "    # 由于python3改变了dict.keys,返回的是dict_keys对象,支持iterable 但不支持indexable，我们可以将其明确的转化成list\n",
    "    firstStr = list(inputTree.keys())[0]\n",
    "    # 通过keys获取values\n",
    "    secondDict = inputTree[firstStr]\n",
    "    # 通过keys， 获取特征列表中的特征的索引\n",
    "    featIndex = featLabels.index(firstStr)\n",
    "    # 通过特征索引获取测试数据中的特征值\n",
    "    key = testVec[featIndex]\n",
    "    # 根据上面获取到的特征值， 确定下一步往树的哪个分支走， 相当于又重新进入到下一个树， 递归上面的步骤。\n",
    "    valueOfFeat = secondDict[key]\n",
    "    if isinstance(valueOfFeat, dict): \n",
    "        classLabel = classify(valueOfFeat, featLabels, testVec)\n",
    "    else: classLabel = valueOfFeat\n",
    "    return classLabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'no'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 分类数据测试\n",
    "data, labels = createDataSet()\n",
    "classify(myTree, labels, [1, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 现在我们拥有了训练好的决策树，我们要把它存储起来，就不用每次都训练一个决策树了\n",
    "# python提供了pickle模块，可以序列化对象，序列化对象可以在磁盘上保存，并在需要的时候读取出来，任何对象都可以进行序列化操作。\n",
    "\n",
    "def storeTree(inputTree,filename):\n",
    "    import pickle\n",
    "    # write() argument must be str, not bytes，修改w为wb，增加'rb'，默认为'r'。\n",
    "    # 原因为：Python3给open函数添加了名为encoding的新参数，而这个新参数的默认值却是‘utf-8’。\n",
    "    # 这样在文件句柄上进行read和write操作时，系统就要求开发者必须传入包含Unicode字符的实例，而不接受包含二进制数据的bytes实例。\n",
    "    fw = open(filename,'wb')\n",
    "    pickle.dump(inputTree,fw)\n",
    "    fw.close()\n",
    "\n",
    "# 加载树    \n",
    "def grabTree(filename):\n",
    "    import pickle\n",
    "    fr = open(filename, 'rb')\n",
    "    return pickle.load(fr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'no surfacing': {0: 'no', 1: {'flippers': {0: 'no', 1: 'yes'}}}}\n",
      "no\n"
     ]
    }
   ],
   "source": [
    "# 测试存储与读取模块\n",
    "\n",
    "myData, labels = createDataSet()\n",
    "myTree = createTree(myData, labels)\n",
    "print (myTree)\n",
    "storeTree(myTree, 'myTree.pkl')\n",
    "\n",
    "\n",
    "myTree = grabTree('myTree.pkl')\n",
    "predict = classify(myTree, ['no surfacing','flippers'], [1,0])\n",
    "print (predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
